{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача\n",
    "Задача - реализовать функционал для анализа степени сходства между заранее известным/и текстом/ами и входной строкой. Степень сходства определить как размер пересечения множества 10 наиболее популярных неуникальных 4-х символьных слов исходного/ых текста/ов и множества всех 4-х символьных слов входной строки.\n",
    "\n",
    "## Решение\n",
    "Решение задачи будет состоять из трёх очевидных этапов: \n",
    "- генерация словаря слов исходного текста или текстов;\n",
    "- генерация словаря слов входной строки;\n",
    "- сравнение схожести строки и каждого из текстов.\n",
    "\n",
    "Для генерации словаря слов использовалось представление текста в виде мешка слов (bag of words или bow). Мешок слов - это модель представления текста в виде множества слов с сохранением информации об их количестве.\n",
    "\n",
    "## Реализация\n",
    "В качестве языка реализации использован язык программирования python версии 3.8.\n",
    "\n",
    "Для решения задачи был реализован класс SimilarityAnalyzer. В нём собраны функции (методы), с помощью которых можно выполнить каждый из этапов, описанных ранее. Использовать класс можно, если импортировать его функционал как часть модуля similarity_analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity_analyzer import SimilarityAnalyzer\n",
    "analyzer = SimilarityAnalyzer() # class to analize similarity of texts and input string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входными данными для программы являются массив исходных текстов (corpus_of_texts) и входная строка (input_text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "corpus_of_texts = [\"zoot crocodiLes diLet azoots\", \"kloot tool stool teloots kept xentool\"]\n",
    "input_text = \"dile kool tool\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же входными данными к программе можно принять не массив исходных текстов, а массив путей до текстовых файлов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_of_texts = [\"data/text1.txt\", \"data/text2.txt\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшего корректного оперирования текстовыми файлами необходимо устанавливать аргумент source='file' в функции генерации мешка слов generate_bow. \n",
    "\n",
    "Итак, выполняем первый этап: генерируем мешки слов для каждого текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bag of words for each text\n",
    "base_bows = []\n",
    "for text in corpus_of_texts:\n",
    "    bow = analyzer.generate_bow(text=text, source='file')\n",
    "    base_bows.append(bow)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Второй этап: генерация мешка слов для входной строки. \n",
    "\n",
    "Следует передать дополнительный аргумент в функцию generate_bow : non_unique_words=False. Так мы получим сгененированный словарь всех слов в строке, а не только не-уникальных. В качестве source используем 'string', чтобы оперировать переменной input_text как строкой, а не как путём к текстовому файлу со строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bow = analyzer.generate_bow(text=input_text, source='string', non_unique_words=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третий этап: анализ схожести. \n",
    "\n",
    "Метод get_similarity возвращает словарь, где ключами являются порядковые номера текстов, а значения в словаре это степень схожести текста с входной строкой. Словарь отсортирован таким образом, что вначале стоят порядковые номера текстов, которые наиболее схожи со входной строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 1}\n"
     ]
    }
   ],
   "source": [
    "# Check the level of similarity between input_text and each text from corpus_of_texts\n",
    "similarity = analyzer.get_similarity(base_bows=base_bows, input_bow=input_bow)\n",
    "\n",
    "print(similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определения наиболее схожего текста необходимо просто взять первый элемент словаря. А если корректнее - первые несколько элементов словаря с одинаковой схожестью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  \n",
      "1  \n"
     ]
    }
   ],
   "source": [
    "most_similar_text_index, max_similarity_degree = list(similarity.items())[0]\n",
    "for text_index, similarity_degree in similarity.items():\n",
    "    if similarity_degree != max_similarity_degree:\n",
    "        break\n",
    "    print(text_index, ' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит упомянуть, что у метода generate_bow есть опциональные аргументы, с помощью которых можно по разному генерировать bag of words (bow).\n",
    "\n",
    "bow_size - максимальный размер сгенерированного словаря (default = 10)\n",
    "\n",
    "word_size - размер слов (default = 4)\n",
    "\n",
    "skip_spaces - пропускать символы пробела при генерации слов или нет (default = True)\n",
    "\n",
    "non_unique_words - гененировать словарь только из неуникальных слов или нет (default = True).\n",
    "\n",
    "У метода 'get_similarity' опциональный аргумент case_sensitive - определять степень схожести с учётом регистра символов в словах или без учета (default = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительно \n",
    "Дополнительно решено было реализовать метод generate_bow_ngram, который использует модель n-gram для генерации bow. Такая модель позволяет учитывать не только информацию о числе вхождений слова, но и пространственную информацию - число вхождений словосочетаний из n слов.\n",
    "\n",
    "Из входного текста извлекаются слова (последовательность символов) между которыми в тексте стоит пробел. Для генерации необходимо учитывать аргумент ngram_range, который задаёт диапазон количества слов в словосочетаниях. Например, устанавливая ngram_range=(1,2) мы сгенерируем bow из слов и словосочетаний из 2-х слов.\n",
    "\n",
    "**Пример использования**\n",
    "\n",
    "Для примера взяты два текста, которые можно классифицировать как \"рецепт\" и \"научный текст\" - data/text_receipt.txt и data/text_wiki.txt соответственно.\n",
    "\n",
    "Предлагается сравнить степень схожести каждого текста со строкой \"oven butter marzipan stir combine flour dough\". Ожидаемый результат - степень схожести текста \"рецепт\" выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4}\n",
      "0  \n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "corpus_of_texts = [\"data/text_receipt.txt\", \"data/text_wiki.txt\"]\n",
    "input_text = \"oven butter marzipan stir combine flour dough\"\n",
    "\n",
    "# Generate bag of words for each text\n",
    "base_bows = []\n",
    "for text in corpus_of_texts:\n",
    "    bow = analyzer.generate_bow_ngram(text=text, source='file', bow_size=50, ngram_range=(1, 1))\n",
    "    base_bows.append(bow)\n",
    "\n",
    "# Check the level of similarity between input_text and each text from corpus_of_texts\n",
    "input_bow = analyzer.generate_bow_ngram(text=input_text, source='string', non_unique_words=False)\n",
    "similarity = analyzer.get_similarity(base_bows=base_bows, input_bow=input_bow)\n",
    "\n",
    "print(similarity)\n",
    "\n",
    "most_similar_text_index, max_similarity_degree = list(similarity.items())[0]\n",
    "for text_index, similarity_degree in similarity.items():\n",
    "    if similarity_degree != max_similarity_degree:\n",
    "        break\n",
    "    print(text_index, ' ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Степень схожести каждого текста со строкой \"information model methods reference example\". Ожидаемый результат - степень схожести текста \"научный текст\" выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3}\n",
      "1  \n"
     ]
    }
   ],
   "source": [
    "input_text = \"information model methods reference example\" \n",
    "input_bow = analyzer.generate_bow_ngram(text=input_text, source='string', non_unique_words=False)\n",
    "similarity = analyzer.get_similarity(base_bows=base_bows, input_bow=input_bow)\n",
    "\n",
    "print(similarity)\n",
    "\n",
    "most_similar_text_index, max_similarity_degree = list(similarity.items())[0]\n",
    "for text_index, similarity_degree in similarity.items():\n",
    "    if similarity_degree != max_similarity_degree:\n",
    "        break\n",
    "    print(text_index, ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
